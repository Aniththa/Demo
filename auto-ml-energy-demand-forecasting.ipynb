{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Energy Demand Forecasting\n",
    "\n",
    "\n",
    "## Problem overview:\n",
    "\n",
    "This scenario focuses on energy demand forecasting where the goal is to predict the future load on an energy grid. It is a critical business operation for companies in the energy sector as operators need to maintain the fine balance between the energy consumed on a grid and the energy supplied to it. Too much power supplied to the grid can result in waste of energy or technical faults. However, if too little power is supplied it can lead to blackouts, leaving customers without power. Typically, grid operators can take short-term decisions to manage energy supply to the grid and keep the load in balance. An accurate short-term forecast of energy demand is therefore essential for the operator to make these decisions with confidence.\n",
    "\n",
    "This scenario details the development of a machine learning energy demand forecasting model. The model is trained on a public dataset from the New York Independent System Operator (NYISO), which operates the power grid for New York State. The dataset includes hourly power demand data for New York City over a period of five years. \n",
    "\n",
    "\n",
    "## Solution overview:\n",
    "\n",
    "We will use the automated ML capability ([what is that?](https://www.youtube.com/watch?v=l8c-4iDPE0M&t=27s)) in [Azure Machine Learning service](https://azure.microsoft.com/en-us/services/machine-learning-service/) to quickly train a model that can predict the future load on an energy grid. This example can be extended to all sorts of forecasting use cases. Automated ML empowers data scientists to identify an end-to-end machine learning pipeline for any problem, and they are able to achieve higher accuracy while spending far less time. \n",
    "\n",
    "1. Basic setup\n",
    "\n",
    "2. Data prep\n",
    "\n",
    "3. Model training\n",
    "\n",
    "4. Explore the results and test the best model \n",
    "\n",
    "5. Model Explainability: which features matter for the forecast?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Basic setup\n",
    "Before starting this step, you need to create an Azure Machine Learning service <b>workspace</b> ([instructions](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace)). \n",
    "\n",
    "Let's get started by creating an experiment in your Azure Machine Learning workspace. An <b>experiment</b> is a named object in a <b>workspace</b>, which is used to do model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Action Required</font>\n",
    "\n",
    "<font color=\"red\"> Replace <subscription_guid> with your Azure Subscription. <resource_group>, <workspace_region>, and <workspace_name> with the values that you would like for this experiment. Make sure you remove the angle brackets. The values should be inside the quotes. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"<subscription id goes here>\"\n",
    "resource_group = \"<resource group goes here>\"\n",
    "workspace_region = \"<workspace region goes here>\"\n",
    "workspace_name = \"<workspace name goes here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Action Required\n",
    "    \n",
    "<font color=\"red\"> Executing next cell will ask you to go to a url to authenticate. Once the authentication is successful, you will see a message that says Interactive authentication successfully completed. In this step you are logging into the Azure subscription </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace(workspace_name = workspace_name,\n",
    "               subscription_id = subscription_id,\n",
    "               resource_group = resource_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'NYCEnergyForecast'\n",
    "\n",
    "# project folder\n",
    "project_folder = './sample_projects/automl-energydemandforecasting'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace Name'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data = output, index = ['']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Data prep\n",
    "nyc_weather.csv contains hourly weather values (nyc_weather.csv contains hourly weather values) for New York City over the same years 2012-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"nyc_energy.csv\", parse_dates=['timeStamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Inspect data\n",
    "Display the first few rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = data.loc[(data.timeStamp>'2016-07-01') & (data.timeStamp<='2016-07-07')]\n",
    "plt.plot(plt_df['timeStamp'], plt_df['demand'])\n",
    "plt.title('New York City power demand over one week in July 2016')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = data.copy().loc[(data['timeStamp']>='2016-01-01') & (data['timeStamp']<'2017-01-01'), ]\n",
    "plt.plot(plt_df['timeStamp'], plt_df['demand'], markersize=1)\n",
    "plt.title('Hourly demand in 2016')\n",
    "plt.ylabel('demand')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take note of what columns means what in the data\n",
    "time_column_name = 'timeStamp'\n",
    "target_column_name = 'demand'\n",
    "\n",
    "X_train = data[data[time_column_name] < '2017-02-01']\n",
    "X_test = data[data[time_column_name] >= '2017-02-01']\n",
    "y_train = X_train.pop(target_column_name).values\n",
    "y_test = X_test.pop(target_column_name).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Model training\n",
    "\n",
    "In this section you will configure automated ML,([What is that?](https://www.youtube.com/watch?v=l8c-4iDPE0M&t=27s)) and run an automated ML experiment which will generate machine learning models. The training jobs are run on VMs provided and managed by Azure Notebooks. \n",
    "\n",
    "What I love the most of automated ML is that even if it accelerates my work as data scientist, I still have total <b>Control</b>, <b>Transparency</b>, <b>Visibility</b> on what I am doing with my data, the training piece and all the metrics it is using to evaluate different ML approaches.\n",
    "\n",
    "Below there is the <b>configuration file</b> for submitting an automated machine learning experiment in Azure Machine Learning service.\n",
    "\n",
    "This configuration object contains and persists the parameters for configuring the experiment run parameters, as well as the training data to be used at run time.\n",
    "\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**iterations**|Number of iterations. In each iteration, Auto ML trains a specific pipeline on the given data|\n",
    "|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n",
    "|**X**|(sparse) array-like, shape = [n_samples, n_features]|\n",
    "|**y**|(sparse) array-like, shape = [n_samples, ], targets values.|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**path**|Relative path to the project folder.  AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Automated ML configuration \n",
    "Automated ML comes with many levers that you can use to configure. This is to give you flexibility and control  \n",
    "For example primary_metric is the metric that Automated ML uses to optimize the machine learning model it is building. Automated ML supports several different primary_metrics. To find out more about all the configuration settings you can leisurely read at ( https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train ) \n",
    "    \n",
    "Notice that the task is set to forecasting. We are also passing the training dataset and validation dataset (X_train, y_train that we prepared in section 2.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"time_column_name\": time_column_name, \n",
    "    \"max_horizon\": 20\n",
    "}\n",
    "\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'forecasting',\n",
    "                             debug_log = 'automl_nyc_energy_errors.log',\n",
    "                             primary_metric= 'normalized_root_mean_squared_error',\n",
    "                             iterations = 5,\n",
    "                             iteration_timeout_minutes = 5,\n",
    "                             X = X_train,\n",
    "                             y = y_train,\n",
    "                             n_cross_validations = 3,\n",
    "                             path=project_folder,\n",
    "                             verbosity = logging.INFO,\n",
    "                            **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train your models on local compute\n",
    "You can call the submit method on the experiment object and pass the AutoMLConfig object you instantiated above. Submit generates a number of machine learning models equivalent to the iterations you set (In this case 5 different models). Depending on the data and number of iterations this can run for while. For local runs the execution is synchronous.\n",
    "You will see the currently running iterations printing to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Monitor training\n",
    "\n",
    "When you execute the cell below, it will render a widget showing the status of all the iterations in a table. The iterations are shown in a leaderboard format with the best performing model being on the top. However you can sort these by clicking on the column heading. If you hover your mouse on any of the pipelines, you will see all the hyper parameters used to build that particular model. If you click on any of the pipelines, a window will popup with a plethora of information including various metrics of the model and a few visual charts. \n",
    "You will also see a step chart showing the metric for each iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the results and test the best model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1 Retrieve all child runs\n",
    "You can fetch all the child runs and see individual metrics that we log for each of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(local_run.get_children())\n",
    "metricslist = {}\n",
    "for run in children:\n",
    "    properties = run.get_properties()\n",
    "    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}    \n",
    "    metricslist[int(properties['iteration'])] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Retrieve best fitted model\n",
    "Below we select the best model from our iterations. \n",
    "\n",
    "The get_output file allows you to retrieve the best run and fitted model for any logged metric or a particular iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "fitted_model.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Test the best fitted model\n",
    "\n",
    "For forecasting, we will use the `forecast` function instead of the `predict` function. There are two reasons for this.\n",
    "\n",
    "We need to pass the recent values of the target variable `y`, whereas the scikit-compatible `predict` function only takes the non-target variables `X`. In our case, the test data immediately follows the training data, and we fill the `y` variable with `NaN`. The `NaN` serves as a question mark for the forecaster to fill with the actuals. Using the forecast function will produce forecasts using the shortest possible forecast horizon. The last time at which a definite (non-NaN) value is seen is the _forecast origin_ - the last time when the value of the target is known. \n",
    "\n",
    "Using the `predict` method would result in getting predictions for EVERY horizon the forecaster can predict at. This is useful when training and evaluating the performance of the forecaster at various horizons, but the level of detail is excessive for normal use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = fitted_model.predict(X_test)\n",
    "y_residual_test = y_test - y_pred_test\n",
    "\n",
    "plt.plot(X_test['timeStamp'], y_test, label='Actual')\n",
    "plt.plot(X_test['timeStamp'], y_pred_test, label=\"Predicted\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Actual demand vs predicted for test data ')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Explainability: Which features matter for the forecast?\n",
    "\n",
    "I can use model explainability with automated ML - This gives me transparency into how the model was built and what features has the most influence on the prediction.  \n",
    " \n",
    "The informative features make all sorts of intuitive sense. Temperature is a strong driver of heating and cooling demand in NYC. Apart from that, the daily life cycle, expressed by `hour`, and the weekly cycle, expressed by `wday` drives people's energy use habits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.automlexplainer import explain_model\n",
    "\n",
    "y_query = y_test.copy().astype(np.float)\n",
    "y_query.fill(np.nan)\n",
    "y_fcst, X_trans = fitted_model.forecast(X_test, y_query)\n",
    "\n",
    "# feature names are everything in the transformed data except the target\n",
    "features = X_trans.columns[:-1]\n",
    "expl = explain_model(fitted_model, X_train, X_test, features = features, best_run=best_run, y_train = y_train)\n",
    "\n",
    "# unpack the tuple\n",
    "shap_values, expected_values, feat_overall_imp, feat_names, per_class_summary, per_class_imp = expl\n",
    "best_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please go to the Azure Portal's best run to see the top features chart.\n",
    "\n",
    "The informative features make all sorts of intuitive sense. Temperature is a strong driver of heating and cooling demand in NYC. Apart from that, the daily life cycle, expressed by `hour`, and the weekly cycle, expressed by `wday` drives people's energy use habits."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "xiaga, tosingli"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
